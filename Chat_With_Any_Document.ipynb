{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/the-h-b/chat_with_pdf/blob/main/Chat_With_Any_Document.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "aMzB_8NCbRJD"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rP3nl-PAbRJE"
      },
      "source": [
        "### Install the requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "RRYSu48huSUW"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "pip -q install langchain faiss-cpu unstructured\n",
        "pip -q install openai tiktoken\n",
        "pip -q install pytesseract pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "-EJIuBbBbRJH"
      },
      "outputs": [],
      "source": [
        "from filetype import guess\n",
        "\n",
        "def detect_document_type(document_path):\n",
        "\n",
        "    guess_file = guess(document_path)\n",
        "    file_type = \"\"\n",
        "    image_types = ['jpg', 'jpeg', 'png', 'gif']\n",
        "\n",
        "    if(guess_file.extension.lower() == \"pdf\"):\n",
        "        file_type = \"pdf\"\n",
        "\n",
        "    elif(guess_file.extension.lower() in image_types):\n",
        "        file_type = \"image\"\n",
        "\n",
        "    else:\n",
        "        file_type = \"unkown\"\n",
        "\n",
        "    return file_type\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EQMV_zObRJI",
        "outputId": "51a8a28d-29fa-4274-b6cc-f957bf66f046"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Research Paper Type: pdf\n",
            "Article Information Document Type: image\n"
          ]
        }
      ],
      "source": [
        "research_paper_path = \"/content/Ghost in the Wires_ My Adventures as the World's Most Wanted Hacker.pdf\"\n",
        "\n",
        "print(f\"Research Paper Type: {detect_document_type(research_paper_path)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "X4x6hvvMbRJI"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders.image import UnstructuredImageLoader\n",
        "from langchain.document_loaders import UnstructuredFileLoader\n",
        "\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(pdf_file):\n",
        "\n",
        "    loader = UnstructuredFileLoader(pdf_file)\n",
        "    documents = loader.load()\n",
        "    pdf_pages_content = '\\n'.join(doc.page_content for doc in documents)\n",
        "\n",
        "    return pdf_pages_content\n",
        "\n",
        "def extract_text_from_image(image_file):\n",
        "\n",
        "    loader = UnstructuredImageLoader(image_file)\n",
        "    documents = loader.load()\n",
        "\n",
        "    image_content = '\\n'.join(doc.page_content for doc in documents)\n",
        "\n",
        "    return image_content\n",
        "\n",
        "\n",
        "\n",
        "def extract_file_content(file_path):\n",
        "\n",
        "    file_type = detect_document_type(file_path)\n",
        "\n",
        "    if(file_type == \"pdf\"):\n",
        "        loader = UnstructuredFileLoader(file_path)\n",
        "\n",
        "    elif(file_type == \"image\"):\n",
        "        loader = UnstructuredImageLoader(file_path)\n",
        "\n",
        "    documents = loader.load()\n",
        "    documents_content = '\\n'.join(doc.page_content for doc in documents)\n",
        "\n",
        "    return documents_content"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cz2EZNJ5gp4D",
        "outputId": "7cfeaa0c-845b-48b2-9379-c9261278523b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unstructured[pdf] in /usr/local/lib/python3.10/dist-packages (0.10.14)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (5.2.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (1.2.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (0.4.27)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (4.9.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (4.11.2)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (2.8.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (0.5.14)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (1.16.3)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (20221105)\n",
            "Requirement already satisfied: unstructured-inference in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (0.5.27)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured[pdf]) (2.5)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured[pdf]) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured[pdf]) (0.9.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[pdf]) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[pdf]) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[pdf]) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[pdf]) (4.66.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image->unstructured[pdf]) (9.4.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured[pdf]) (3.2.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured[pdf]) (41.0.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[pdf]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[pdf]) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[pdf]) (2023.7.22)\n",
            "Requirement already satisfied: layoutparser[layoutmodels,tesseract] in /usr/local/lib/python3.10/dist-packages (from unstructured-inference->unstructured[pdf]) (0.3.4)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from unstructured-inference->unstructured[pdf]) (0.0.6)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from unstructured-inference->unstructured[pdf]) (0.17.1)\n",
            "Requirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference->unstructured[pdf]) (4.8.0.76)\n",
            "Requirement already satisfied: onnx==1.14.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference->unstructured[pdf]) (1.14.1)\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.10/dist-packages (from unstructured-inference->unstructured[pdf]) (1.15.1)\n",
            "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference->unstructured[pdf]) (4.33.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx==1.14.1->unstructured-inference->unstructured[pdf]) (1.23.5)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx==1.14.1->unstructured-inference->unstructured[pdf]) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.10/dist-packages (from onnx==1.14.1->unstructured-inference->unstructured[pdf]) (4.5.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->unstructured[pdf]) (1.15.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->unstructured[pdf]) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference->unstructured[pdf]) (3.12.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference->unstructured[pdf]) (6.0.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference->unstructured[pdf]) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference->unstructured[pdf]) (0.3.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->unstructured-inference->unstructured[pdf]) (2023.6.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured[pdf]) (1.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference->unstructured[pdf]) (1.11.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference->unstructured[pdf]) (1.5.3)\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference->unstructured[pdf]) (0.1.10)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference->unstructured[pdf]) (0.10.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference->unstructured[pdf]) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference->unstructured[pdf]) (0.15.2+cu118)\n",
            "Requirement already satisfied: effdet in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference->unstructured[pdf]) (0.4.1)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference->unstructured[pdf]) (0.3.10)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime->unstructured-inference->unstructured[pdf]) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime->unstructured-inference->unstructured[pdf]) (23.5.26)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime->unstructured-inference->unstructured[pdf]) (1.12)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured[pdf]) (2.21)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime->unstructured-inference->unstructured[pdf]) (10.0)\n",
            "Requirement already satisfied: timm>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference->unstructured[pdf]) (0.9.7)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference->unstructured[pdf]) (2.0.7)\n",
            "Requirement already satisfied: omegaconf>=2.0 in /usr/local/lib/python3.10/dist-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference->unstructured[pdf]) (2.3.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference->unstructured[pdf]) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference->unstructured[pdf]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference->unstructured[pdf]) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->layoutparser[layoutmodels,tesseract]->unstructured-inference->unstructured[pdf]) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->layoutparser[layoutmodels,tesseract]->unstructured-inference->unstructured[pdf]) (16.0.6)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath->layoutparser[layoutmodels,tesseract]->unstructured-inference->unstructured[pdf]) (2.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->layoutparser[layoutmodels,tesseract]->unstructured-inference->unstructured[pdf]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->layoutparser[layoutmodels,tesseract]->unstructured-inference->unstructured[pdf]) (2023.3.post1)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured-inference->unstructured[pdf]) (4.20.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime->unstructured-inference->unstructured[pdf]) (1.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf>=2.0->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference->unstructured[pdf]) (4.9.3)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference->unstructured[pdf]) (3.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->layoutparser[layoutmodels,tesseract]->unstructured-inference->unstructured[pdf]) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->layoutparser[layoutmodels,tesseract]->unstructured-inference->unstructured[pdf]) (2.1.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference->unstructured[pdf]) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference->unstructured[pdf]) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference->unstructured[pdf]) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference->unstructured[pdf]) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference->unstructured[pdf]) (3.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber\n",
        "!pip install PyMuPDF\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EE9mVJetgxWf",
        "outputId": "5973ee0d-dccd-40b9-af8b-9798686bcac4"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (0.10.2)\n",
            "Requirement already satisfied: pdfminer.six==20221105 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (20221105)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (9.4.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (4.20.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105->pdfplumber) (3.2.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105->pdfplumber) (41.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (2.21)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.10/dist-packages (1.23.3)\n",
            "Requirement already satisfied: PyMuPDFb==1.23.3 in /usr/local/lib/python3.10/dist-packages (from PyMuPDF) (1.23.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgA3LcDTbRJJ",
        "outputId": "bfa1fb0d-4406-4924-b69f-91a41dc6bce1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Providedproperattributionisprovided,Googleherebygrantspermissionto\n",
            "reproducethetablesandfiguresinthispapersolelyforuseinjournalisticor\n",
            "scholarlyworks.\n",
            "Attention Is All You Need\n",
            "3202\n",
            "AshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\n",
            "GoogleBrain GoogleBrain GoogleResearch GoogleResearch\n",
            "avaswani@google.com noam@google.com nikip@google.com usz@google.com\n",
            "luJ\n",
            "LlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\n",
            "GoogleResearch UniversityofToronto GoogleBrain 42\n",
            "llion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\n",
            "]LC.sc[\n",
            "IlliaPolosukhin∗ ‡\n",
            "illia.polosukhin@gmail.com\n",
            "Abstract\n",
            "6v26730.6071:viXra\n",
            "Thedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\n",
            "convolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\n",
            "performing models also connect the encoder and decoder through an attention\n",
            "mechanism. We propose a new simple network architecture, the Transformer,\n",
            "basedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\n",
            "entirely. Experiments on two machine translation tasks show these models to\n",
            "besuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\n",
            "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
            "to-German translation task, improving over the existing best results, including\n",
            "ensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\n",
            "ourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after\n",
            "trainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\n",
            "bestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto\n",
            "othertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\n",
            "largeandlimitedtrainingdata.\n",
            "∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\n",
            "theefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\n",
            "hasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\n",
            "attentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvolvedinnearlyevery\n",
            "detail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\n",
            "tensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\n",
            "efficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\n",
            "implementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\n",
            "ourresearch.\n",
            "†WorkperformedwhileatGoogleBrain.\n",
            "‡WorkperformedwhileatGoogleResearch.\n",
            "31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.1 Introduction\n",
            "Recurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks\n",
            "inparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\n",
            "transductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous\n",
            "effortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\n",
            "architectures[38,24,15].\n",
            "Recurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\n",
            "sequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\n",
            "statesh ,asafunctionoftheprevioushiddenstateh andtheinputforpositiont. Thisinherently\n",
            "t t−1\n",
            "sequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\n",
            "sequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\n",
            "significantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\n",
            "computation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\n",
            "constraintofsequentialcomputation,however,remains.\n",
            "Attentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\n",
            "tionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\n",
            "theinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\n",
            "areusedinconjunctionwitharecurrentnetwork.\n",
            "InthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\n",
            "relyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\n",
            "TheTransformerallowsforsignificantlymoreparallelizationandcanreachanewstateoftheartin\n",
            "translationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n",
            "2 Background\n",
            "ThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n",
            "[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\n",
            "block,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\n",
            "thenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\n",
            "inthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\n",
            "it more difficult to learn dependencies between distant positions [12]. In the Transformer this is\n",
            "reducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\n",
            "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
            "describedinsection3.2.\n",
            "Self-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\n",
            "ofasinglesequenceinordertocomputearepresentationofthesequence. Self-attentionhasbeen\n",
            "usedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\n",
            "textualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\n",
            "End-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\n",
            "alignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\n",
            "languagemodelingtasks[34].\n",
            "To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
            "entirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\n",
            "alignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\n",
            "self-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n",
            "3 ModelArchitecture\n",
            "Mostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].\n",
            "Here, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\n",
            "1 n\n",
            "of continuous representations z = (z ,...,z ). Given z, the decoder then generates an output\n",
            "1 n\n",
            "sequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\n",
            "1 m\n",
            "[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\n",
            "2Figure1: TheTransformer-modelarchitecture.\n",
            "TheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\n",
            "connectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\n",
            "respectively.\n",
            "3.1 EncoderandDecoderStacks\n",
            "Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\n",
            "sub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\n",
            "wisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof\n",
            "the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\n",
            "LayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\n",
            "itself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\n",
            "layers,produceoutputsofdimensiond =512.\n",
            "model\n",
            "Decoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\n",
            "sub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\n",
            "attentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\n",
            "aroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\n",
            "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
            "masking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\n",
            "predictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\n",
            "3.2 Attention\n",
            "Anattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\n",
            "wherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum\n",
            "3ScaledDot-ProductAttention Multi-HeadAttention\n",
            "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\n",
            "attentionlayersrunninginparallel.\n",
            "ofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\n",
            "querywiththecorrespondingkey.\n",
            "3.2.1 ScaledDot-ProductAttention\n",
            "Wecallourparticularattention\"ScaledDot-ProductAttention\"(Figure2). Theinputconsistsof\n",
            "queriesandkeysofdimensiond k,a√ndvaluesofdimensiond v. Wecomputethedotproductsofthe\n",
            "querywithallkeys,divideeachby d ,andapplyasoftmaxfunctiontoobtaintheweightsonthe\n",
            "k\n",
            "values.\n",
            "Inpractice,wecomputetheattentionfunctiononasetofqueriessimultaneously,packedtogether\n",
            "intoamatrixQ. ThekeysandvaluesarealsopackedtogetherintomatricesK andV. Wecompute\n",
            "thematrixofoutputsas:\n",
            "QKT\n",
            "Attention(Q,K,V)=softmax( √ )V (1)\n",
            "d\n",
            "k\n",
            "Thetwomostcommonlyusedattentionfunctionsareadditiveattention[2],anddot-product(multi-\n",
            "plicative)attention. Dot-productattentionisidenticaltoouralgorithm,exceptforthescalingfactor\n",
            "of √1 . Additiveattentioncomputesthecompatibilityfunctionusingafeed-forwardnetworkwith\n",
            "dk\n",
            "asinglehiddenlayer. Whilethetwoaresimilarintheoreticalcomplexity,dot-productattentionis\n",
            "muchfasterandmorespace-efficientinpractice,sinceitcanbeimplementedusinghighlyoptimized\n",
            "matrixmultiplicationcode.\n",
            "Whileforsmallvaluesofd thetwomechanismsperformsimilarly,additiveattentionoutperforms\n",
            "k\n",
            "dotproductattentionwithoutscalingforlargervaluesofd [3]. Wesuspectthatforlargevaluesof\n",
            "k\n",
            "d ,thedotproductsgrowlargeinmagnitude,pushingthesoftmaxfunctionintoregionswhereithas\n",
            "k\n",
            "extremelysmallgradients4. Tocounteractthiseffect,wescalethedotproductsby √1 .\n",
            "dk\n",
            "3.2.2 Multi-HeadAttention\n",
            "Insteadofperformingasingleattentionfunctionwithd -dimensionalkeys,valuesandqueries,\n",
            "model\n",
            "wefounditbeneficialtolinearlyprojectthequeries,keysandvalueshtimeswithdifferent,learned\n",
            "linearprojectionstod ,d andd dimensions,respectively. Oneachoftheseprojectedversionsof\n",
            "k k v\n",
            "queries,keysandvalueswethenperformtheattentionfunctioninparallel,yieldingd -dimensional\n",
            "v\n",
            "4Toillustratewhythedotproductsgetlarge,assumethatthecomponentsofqandkareindependentrandom\n",
            "variableswithmean0andvariance1.Thentheirdotproduct,q·k=(cid:80)dk\n",
            "q k ,hasmean0andvarianced .\n",
            "i=1 i i k\n",
            "4output values. These are concatenated and once again projected, resulting in the final values, as\n",
            "depictedinFigure2.\n",
            "Multi-headattentionallowsthemodeltojointlyattendtoinformationfromdifferentrepresentation\n",
            "subspacesatdifferentpositions. Withasingleattentionhead,averaginginhibitsthis.\n",
            "MultiHead(Q,K,V)=Concat(head ,...,head )WO\n",
            "1 h\n",
            "wherehead =Attention(QWQ,KWK,VWV)\n",
            "i i i i\n",
            "WheretheprojectionsareparametermatricesWQ ∈Rdmodel×dk,WK ∈Rdmodel×dk,WV ∈Rdmodel×dv\n",
            "i i i\n",
            "andWO ∈Rhdv×dmodel.\n",
            "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\n",
            "d =d =d /h=64. Duetothereduceddimensionofeachhead,thetotalcomputationalcost\n",
            "k v model\n",
            "issimilartothatofsingle-headattentionwithfulldimensionality.\n",
            "3.2.3 ApplicationsofAttentioninourModel\n",
            "TheTransformerusesmulti-headattentioninthreedifferentways:\n",
            "• In\"encoder-decoderattention\"layers,thequeriescomefromthepreviousdecoderlayer,\n",
            "andthememorykeysandvaluescomefromtheoutputoftheencoder. Thisallowsevery\n",
            "positioninthedecodertoattendoverallpositionsintheinputsequence. Thismimicsthe\n",
            "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
            "[38,2,9].\n",
            "• Theencodercontainsself-attentionlayers. Inaself-attentionlayerallofthekeys,values\n",
            "andqueriescomefromthesameplace,inthiscase,theoutputofthepreviouslayerinthe\n",
            "encoder. Eachpositionintheencodercanattendtoallpositionsinthepreviouslayerofthe\n",
            "encoder.\n",
            "• Similarly,self-attentionlayersinthedecoderalloweachpositioninthedecodertoattendto\n",
            "allpositionsinthedecoderuptoandincludingthatposition. Weneedtopreventleftward\n",
            "informationflowinthedecodertopreservetheauto-regressiveproperty. Weimplementthis\n",
            "insideofscaleddot-productattentionbymaskingout(settingto−∞)allvaluesintheinput\n",
            "ofthesoftmaxwhichcorrespondtoillegalconnections. SeeFigure2.\n",
            "3.3 Position-wiseFeed-ForwardNetworks\n",
            "Inadditiontoattentionsub-layers,eachofthelayersinourencoderanddecodercontainsafully\n",
            "connectedfeed-forwardnetwork,whichisappliedtoeachpositionseparatelyandidentically. This\n",
            "consistsoftwolineartransformationswithaReLUactivationinbetween.\n",
            "FFN(x)=max(0,xW +b )W +b (2)\n",
            "1 1 2 2\n",
            "Whilethelineartransformationsarethesameacrossdifferentpositions,theyusedifferentparameters\n",
            "from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n",
            "The dimensionality of input and output is d = 512, and the inner-layer has dimensionality\n",
            "model\n",
            "d =2048.\n",
            "ff\n",
            "3.4 EmbeddingsandSoftmax\n",
            "Similarlytoothersequencetransductionmodels,weuselearnedembeddingstoconverttheinput\n",
            "tokensandoutputtokenstovectorsofdimensiond . Wealsousetheusuallearnedlineartransfor-\n",
            "model\n",
            "mationandsoftmaxfunctiontoconvertthedecoderoutputtopredictednext-tokenprobabilities. In\n",
            "ourmodel,wesharethesameweightmatrixbetweenthetwoembeddinglayersandthepre-√softmax\n",
            "lineartransformation,similarto[30]. Intheembeddinglayers,wemultiplythoseweightsby d .\n",
            "model\n",
            "5Table1: Maximumpathlengths,per-layercomplexityandminimumnumberofsequentialoperations\n",
            "fordifferentlayertypes. nisthesequencelength,distherepresentationdimension,kisthekernel\n",
            "sizeofconvolutionsandrthesizeoftheneighborhoodinrestrictedself-attention.\n",
            "LayerType ComplexityperLayer Sequential MaximumPathLength\n",
            "Operations\n",
            "Self-Attention O(n2·d) O(1) O(1)\n",
            "Recurrent O(n·d2) O(n) O(n)\n",
            "Convolutional O(k·n·d2) O(1) O(log (n))\n",
            "k\n",
            "Self-Attention(restricted) O(r·n·d) O(1) O(n/r)\n",
            "3.5 PositionalEncoding\n",
            "Sinceourmodelcontainsnorecurrenceandnoconvolution,inorderforthemodeltomakeuseofthe\n",
            "orderofthesequence,wemustinjectsomeinformationabouttherelativeorabsolutepositionofthe\n",
            "tokensinthesequence. Tothisend,weadd\"positionalencodings\"totheinputembeddingsatthe\n",
            "bottomsoftheencoderanddecoderstacks. Thepositionalencodingshavethesamedimensiond\n",
            "model\n",
            "astheembeddings,sothatthetwocanbesummed. Therearemanychoicesofpositionalencodings,\n",
            "learnedandfixed[9].\n",
            "Inthiswork,weusesineandcosinefunctionsofdifferentfrequencies:\n",
            "PE =sin(pos/100002i/dmodel)\n",
            "(pos,2i)\n",
            "PE =cos(pos/100002i/dmodel)\n",
            "(pos,2i+1)\n",
            "whereposisthepositionandiisthedimension. Thatis,eachdimensionofthepositionalencoding\n",
            "correspondstoasinusoid. Thewavelengthsformageometricprogressionfrom2πto10000·2π. We\n",
            "chosethisfunctionbecausewehypothesizeditwouldallowthemodeltoeasilylearntoattendby\n",
            "relativepositions,sinceforanyfixedoffsetk,PE canberepresentedasalinearfunctionof\n",
            "pos+k\n",
            "PE .\n",
            "pos\n",
            "Wealsoexperimentedwithusinglearnedpositionalembeddings[9]instead,andfoundthatthetwo\n",
            "versionsproducednearlyidenticalresults(seeTable3row(E)).Wechosethesinusoidalversion\n",
            "becauseitmayallowthemodeltoextrapolatetosequencelengthslongerthantheonesencountered\n",
            "duringtraining.\n",
            "4 WhySelf-Attention\n",
            "In this section we compare various aspects of self-attention layers to the recurrent and convolu-\n",
            "tionallayerscommonlyusedformappingonevariable-lengthsequenceofsymbolrepresentations\n",
            "(x ,...,x ) to another sequence of equal length (z ,...,z ), with x ,z ∈ Rd, such as a hidden\n",
            "1 n 1 n i i\n",
            "layerinatypicalsequencetransductionencoderordecoder. Motivatingouruseofself-attentionwe\n",
            "considerthreedesiderata.\n",
            "Oneisthetotalcomputationalcomplexityperlayer. Anotheristheamountofcomputationthatcan\n",
            "beparallelized,asmeasuredbytheminimumnumberofsequentialoperationsrequired.\n",
            "Thethirdisthepathlengthbetweenlong-rangedependenciesinthenetwork. Learninglong-range\n",
            "dependenciesisakeychallengeinmanysequencetransductiontasks. Onekeyfactoraffectingthe\n",
            "abilitytolearnsuchdependenciesisthelengthofthepathsforwardandbackwardsignalshaveto\n",
            "traverseinthenetwork. Theshorterthesepathsbetweenanycombinationofpositionsintheinput\n",
            "andoutputsequences,theeasieritistolearnlong-rangedependencies[12]. Hencewealsocompare\n",
            "themaximumpathlengthbetweenanytwoinputandoutputpositionsinnetworkscomposedofthe\n",
            "differentlayertypes.\n",
            "AsnotedinTable1,aself-attentionlayerconnectsallpositionswithaconstantnumberofsequentially\n",
            "executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\n",
            "computationalcomplexity,self-attentionlayersarefasterthanrecurrentlayerswhenthesequence\n",
            "6length n is smaller than the representation dimensionality d, which is most often the case with\n",
            "sentencerepresentationsusedbystate-of-the-artmodelsinmachinetranslations,suchasword-piece\n",
            "[38]andbyte-pair[31]representations. Toimprovecomputationalperformancefortasksinvolving\n",
            "verylongsequences,self-attentioncouldberestrictedtoconsideringonlyaneighborhoodofsizerin\n",
            "theinputsequencecenteredaroundtherespectiveoutputposition. Thiswouldincreasethemaximum\n",
            "pathlengthtoO(n/r). Weplantoinvestigatethisapproachfurtherinfuturework.\n",
            "Asingleconvolutionallayerwithkernelwidthk <ndoesnotconnectallpairsofinputandoutput\n",
            "positions. DoingsorequiresastackofO(n/k)convolutionallayersinthecaseofcontiguouskernels,\n",
            "orO(log (n))inthecaseofdilatedconvolutions[18], increasingthelengthofthelongestpaths\n",
            "k\n",
            "betweenanytwopositionsinthenetwork. Convolutionallayersaregenerallymoreexpensivethan\n",
            "recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\n",
            "considerably, toO(k·n·d+n·d2). Evenwithk = n, however, thecomplexityofaseparable\n",
            "convolutionisequaltothecombinationofaself-attentionlayerandapoint-wisefeed-forwardlayer,\n",
            "theapproachwetakeinourmodel.\n",
            "Assidebenefit,self-attentioncouldyieldmoreinterpretablemodels.Weinspectattentiondistributions\n",
            "fromourmodelsandpresentanddiscussexamplesintheappendix. Notonlydoindividualattention\n",
            "headsclearlylearntoperformdifferenttasks,manyappeartoexhibitbehaviorrelatedtothesyntactic\n",
            "andsemanticstructureofthesentences.\n",
            "5 Training\n",
            "Thissectiondescribesthetrainingregimeforourmodels.\n",
            "5.1 TrainingDataandBatching\n",
            "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\n",
            "sentencepairs. Sentenceswereencodedusingbyte-pairencoding[3],whichhasasharedsource-\n",
            "targetvocabularyofabout37000tokens. ForEnglish-French,weusedthesignificantlylargerWMT\n",
            "2014English-Frenchdatasetconsistingof36Msentencesandsplittokensintoa32000word-piece\n",
            "vocabulary[38].Sentencepairswerebatchedtogetherbyapproximatesequencelength.Eachtraining\n",
            "batchcontainedasetofsentencepairscontainingapproximately25000sourcetokensand25000\n",
            "targettokens.\n",
            "5.2 HardwareandSchedule\n",
            "Wetrainedourmodelsononemachinewith8NVIDIAP100GPUs. Forourbasemodelsusing\n",
            "thehyperparametersdescribedthroughoutthepaper,eachtrainingsteptookabout0.4seconds. We\n",
            "trainedthebasemodelsforatotalof100,000stepsor12hours. Forourbigmodels,(describedonthe\n",
            "bottomlineoftable3),steptimewas1.0seconds. Thebigmodelsweretrainedfor300,000steps\n",
            "(3.5days).\n",
            "5.3 Optimizer\n",
            "WeusedtheAdamoptimizer[20]withβ =0.9,β =0.98andϵ=10−9. Wevariedthelearning\n",
            "1 2\n",
            "rateoverthecourseoftraining,accordingtotheformula:\n",
            "lrate=d−0.5 ·min(step_num−0.5,step_num·warmup_steps−1.5) (3)\n",
            "model\n",
            "Thiscorrespondstoincreasingthelearningratelinearlyforthefirstwarmup_stepstrainingsteps,\n",
            "anddecreasingitthereafterproportionallytotheinversesquarerootofthestepnumber. Weused\n",
            "warmup_steps=4000.\n",
            "5.4 Regularization\n",
            "Weemploythreetypesofregularizationduringtraining:\n",
            "7Table2: TheTransformerachievesbetterBLEUscoresthanpreviousstate-of-the-artmodelsonthe\n",
            "English-to-GermanandEnglish-to-Frenchnewstest2014testsatafractionofthetrainingcost.\n",
            "BLEU TrainingCost(FLOPs)\n",
            "Model\n",
            "EN-DE EN-FR EN-DE EN-FR\n",
            "ByteNet[18] 23.75\n",
            "Deep-Att+PosUnk[39] 39.2 1.0·1020\n",
            "GNMT+RL[38] 24.6 39.92 2.3·1019 1.4·1020\n",
            "ConvS2S[9] 25.16 40.46 9.6·1018 1.5·1020\n",
            "MoE[32] 26.03 40.56 2.0·1019 1.2·1020\n",
            "Deep-Att+PosUnkEnsemble[39] 40.4 8.0·1020\n",
            "GNMT+RLEnsemble[38] 26.30 41.16 1.8·1020 1.1·1021\n",
            "ConvS2SEnsemble[9] 26.36 41.29 7.7·1019 1.2·1021\n",
            "Transformer(basemodel) 27.3 38.1 3.3·1018\n",
            "Transformer(big) 28.4 41.8 2.3·1019\n",
            "ResidualDropout Weapplydropout[33]totheoutputofeachsub-layer,beforeitisaddedtothe\n",
            "sub-layerinputandnormalized. Inaddition,weapplydropouttothesumsoftheembeddingsandthe\n",
            "positionalencodingsinboththeencoderanddecoderstacks. Forthebasemodel,weusearateof\n",
            "P =0.1.\n",
            "drop\n",
            "LabelSmoothing Duringtraining,weemployedlabelsmoothingofvalueϵ = 0.1[36]. This\n",
            "ls\n",
            "hurtsperplexity,asthemodellearnstobemoreunsure,butimprovesaccuracyandBLEUscore.\n",
            "6 Results\n",
            "6.1 MachineTranslation\n",
            "OntheWMT2014English-to-Germantranslationtask,thebigtransformermodel(Transformer(big)\n",
            "inTable2)outperformsthebestpreviouslyreportedmodels(includingensembles)bymorethan2.0\n",
            "BLEU,establishinganewstate-of-the-artBLEUscoreof28.4. Theconfigurationofthismodelis\n",
            "listedinthebottomlineofTable3. Trainingtook3.5dayson8P100GPUs. Evenourbasemodel\n",
            "surpassesallpreviouslypublishedmodelsandensembles,atafractionofthetrainingcostofanyof\n",
            "thecompetitivemodels.\n",
            "OntheWMT2014English-to-Frenchtranslationtask,ourbigmodelachievesaBLEUscoreof41.0,\n",
            "outperformingallofthepreviouslypublishedsinglemodels,atlessthan1/4thetrainingcostofthe\n",
            "previousstate-of-the-artmodel. TheTransformer(big)modeltrainedforEnglish-to-Frenchused\n",
            "dropoutrateP =0.1,insteadof0.3.\n",
            "drop\n",
            "Forthebasemodels,weusedasinglemodelobtainedbyaveragingthelast5checkpoints,which\n",
            "werewrittenat10-minuteintervals. Forthebigmodels,weaveragedthelast20checkpoints. We\n",
            "usedbeamsearchwithabeamsizeof4andlengthpenaltyα = 0.6[38]. Thesehyperparameters\n",
            "werechosenafterexperimentationonthedevelopmentset. Wesetthemaximumoutputlengthduring\n",
            "inferencetoinputlength+50,butterminateearlywhenpossible[38].\n",
            "Table2summarizesourresultsandcomparesourtranslationqualityandtrainingcoststoothermodel\n",
            "architecturesfromtheliterature. Weestimatethenumberoffloatingpointoperationsusedtotraina\n",
            "modelbymultiplyingthetrainingtime,thenumberofGPUsused,andanestimateofthesustained\n",
            "single-precisionfloating-pointcapacityofeachGPU5.\n",
            "6.2 ModelVariations\n",
            "ToevaluatetheimportanceofdifferentcomponentsoftheTransformer,wevariedourbasemodel\n",
            "indifferentways,measuringthechangeinperformanceonEnglish-to-Germantranslationonthe\n",
            "5Weusedvaluesof2.8,3.7,6.0and9.5TFLOPSforK80,K40,M40andP100,respectively.\n",
            "8Table3: VariationsontheTransformerarchitecture. Unlistedvaluesareidenticaltothoseofthebase\n",
            "model. AllmetricsareontheEnglish-to-Germantranslationdevelopmentset,newstest2013. Listed\n",
            "perplexitiesareper-wordpiece,accordingtoourbyte-pairencoding,andshouldnotbecomparedto\n",
            "per-wordperplexities.\n",
            "train PPL BLEU params\n",
            "N d d h d d P ϵ\n",
            "model ff k v drop ls steps (dev) (dev) ×106\n",
            "base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n",
            "1 512 512 5.29 24.9\n",
            "4 128 128 5.00 25.5\n",
            "(A)\n",
            "16 32 32 4.91 25.8\n",
            "32 16 16 5.01 25.4\n",
            "16 5.16 25.1 58\n",
            "(B)\n",
            "32 5.01 25.4 60\n",
            "2 6.11 23.7 36\n",
            "4 5.19 25.3 50\n",
            "8 4.88 25.5 80\n",
            "(C) 256 32 32 5.75 24.5 28\n",
            "1024 128 128 4.66 26.0 168\n",
            "1024 5.12 25.4 53\n",
            "4096 4.75 26.2 90\n",
            "0.0 5.77 24.6\n",
            "0.2 4.95 25.5\n",
            "(D)\n",
            "0.0 4.67 25.3\n",
            "0.2 5.47 25.7\n",
            "(E) positionalembeddinginsteadofsinusoids 4.92 25.7\n",
            "big 6 1024 4096 16 0.3 300K 4.33 26.4 213\n",
            "developmentset,newstest2013. Weusedbeamsearchasdescribedintheprevioussection,butno\n",
            "checkpointaveraging. WepresenttheseresultsinTable3.\n",
            "InTable3rows(A),wevarythenumberofattentionheadsandtheattentionkeyandvaluedimensions,\n",
            "keeping the amount of computation constant, as described in Section 3.2.2. While single-head\n",
            "attentionis0.9BLEUworsethanthebestsetting,qualityalsodropsoffwithtoomanyheads.\n",
            "InTable3rows(B),weobservethatreducingtheattentionkeysized hurtsmodelquality. This\n",
            "k\n",
            "suggests that determining compatibility is not easy and that a more sophisticated compatibility\n",
            "functionthandotproductmaybebeneficial. Wefurtherobserveinrows(C)and(D)that,asexpected,\n",
            "biggermodelsarebetter,anddropoutisveryhelpfulinavoidingover-fitting.Inrow(E)wereplaceour\n",
            "sinusoidalpositionalencodingwithlearnedpositionalembeddings[9],andobservenearlyidentical\n",
            "resultstothebasemodel.\n",
            "6.3 EnglishConstituencyParsing\n",
            "ToevaluateiftheTransformercangeneralizetoothertasksweperformedexperimentsonEnglish\n",
            "constituencyparsing. Thistaskpresentsspecificchallenges: theoutputissubjecttostrongstructural\n",
            "constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\n",
            "modelshavenotbeenabletoattainstate-of-the-artresultsinsmall-dataregimes[37].\n",
            "Wetraineda4-layertransformerwithd =1024ontheWallStreetJournal(WSJ)portionofthe\n",
            "model\n",
            "PennTreebank[25],about40Ktrainingsentences. Wealsotraineditinasemi-supervisedsetting,\n",
            "usingthelargerhigh-confidenceandBerkleyParsercorporafromwithapproximately17Msentences\n",
            "[37]. Weusedavocabularyof16KtokensfortheWSJonlysettingandavocabularyof32Ktokens\n",
            "forthesemi-supervisedsetting.\n",
            "Weperformedonlyasmallnumberofexperimentstoselectthedropout,bothattentionandresidual\n",
            "(section5.4),learningratesandbeamsizeontheSection22developmentset,allotherparameters\n",
            "remained unchanged from the English-to-German base translation model. During inference, we\n",
            "9Table4: TheTransformergeneralizeswelltoEnglishconstituencyparsing(ResultsareonSection23\n",
            "ofWSJ)\n",
            "Parser Training WSJ23F1\n",
            "Vinyals&Kaiserelal. (2014)[37] WSJonly,discriminative 88.3\n",
            "Petrovetal. (2006)[29] WSJonly,discriminative 90.4\n",
            "Zhuetal. (2013)[40] WSJonly,discriminative 90.4\n",
            "Dyeretal. (2016)[8] WSJonly,discriminative 91.7\n",
            "Transformer(4layers) WSJonly,discriminative 91.3\n",
            "Zhuetal. (2013)[40] semi-supervised 91.3\n",
            "Huang&Harper(2009)[14] semi-supervised 91.3\n",
            "McCloskyetal. (2006)[26] semi-supervised 92.1\n",
            "Vinyals&Kaiserelal. (2014)[37] semi-supervised 92.1\n",
            "Transformer(4layers) semi-supervised 92.7\n",
            "Luongetal. (2015)[23] multi-task 93.0\n",
            "Dyeretal. (2016)[8] generative 93.3\n",
            "increasedthemaximumoutputlengthtoinputlength+300. Weusedabeamsizeof21andα=0.3\n",
            "forbothWSJonlyandthesemi-supervisedsetting.\n",
            "Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\n",
            "prisinglywell,yieldingbetterresultsthanallpreviouslyreportedmodelswiththeexceptionofthe\n",
            "RecurrentNeuralNetworkGrammar[8].\n",
            "IncontrasttoRNNsequence-to-sequencemodels[37],theTransformeroutperformstheBerkeley-\n",
            "Parser[29]evenwhentrainingonlyontheWSJtrainingsetof40Ksentences.\n",
            "7 Conclusion\n",
            "Inthiswork,wepresentedtheTransformer,thefirstsequencetransductionmodelbasedentirelyon\n",
            "attention,replacingtherecurrentlayersmostcommonlyusedinencoder-decoderarchitectureswith\n",
            "multi-headedself-attention.\n",
            "For translation tasks, the Transformer can be trained significantly faster than architectures based\n",
            "on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\n",
            "English-to-Frenchtranslationtasks,weachieveanewstateoftheart. Intheformertaskourbest\n",
            "modeloutperformsevenallpreviouslyreportedensembles.\n",
            "Weareexcitedaboutthefutureofattention-basedmodelsandplantoapplythemtoothertasks. We\n",
            "plantoextendtheTransformertoproblemsinvolvinginputandoutputmodalitiesotherthantextand\n",
            "toinvestigatelocal,restrictedattentionmechanismstoefficientlyhandlelargeinputsandoutputs\n",
            "suchasimages,audioandvideo. Makinggenerationlesssequentialisanotherresearchgoalsofours.\n",
            "The code we used to train and evaluate our models is available at https://github.com/\n",
            "tensorflow/tensor2tensor.\n",
            "Acknowledgements WearegratefultoNalKalchbrennerandStephanGouwsfortheirfruitful\n",
            "comments,correctionsandinspiration.\n",
            "References\n",
            "[1] JimmyLeiBa,JamieRyanKiros,andGeoffreyEHinton. Layernormalization. arXivpreprint\n",
            "arXiv:1607.06450,2016.\n",
            "[2] DzmitryBahdanau,KyunghyunCho,andYoshuaBengio. Neuralmachinetranslationbyjointly\n",
            "learningtoalignandtranslate. CoRR,abs/1409.0473,2014.\n",
            "[3] DennyBritz,AnnaGoldie,Minh-ThangLuong,andQuocV.Le. Massiveexplorationofneural\n",
            "machinetranslationarchitectures. CoRR,abs/1703.03906,2017.\n",
            "[4] JianpengCheng,LiDong,andMirellaLapata. Longshort-termmemory-networksformachine\n",
            "reading. arXivpreprintarXiv:1601.06733,2016.\n",
            "10[5] KyunghyunCho,BartvanMerrienboer,CaglarGulcehre,FethiBougares,HolgerSchwenk,\n",
            "andYoshuaBengio. Learningphraserepresentationsusingrnnencoder-decoderforstatistical\n",
            "machinetranslation. CoRR,abs/1406.1078,2014.\n",
            "[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\n",
            "preprintarXiv:1610.02357,2016.\n",
            "[7] JunyoungChung,ÇaglarGülçehre,KyunghyunCho,andYoshuaBengio. Empiricalevaluation\n",
            "ofgatedrecurrentneuralnetworksonsequencemodeling. CoRR,abs/1412.3555,2014.\n",
            "[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\n",
            "networkgrammars. InProc.ofNAACL,2016.\n",
            "[9] JonasGehring,MichaelAuli,DavidGrangier,DenisYarats,andYannN.Dauphin. Convolu-\n",
            "tionalsequencetosequencelearning. arXivpreprintarXiv:1705.03122v2,2017.\n",
            "[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\n",
            "arXiv:1308.0850,2013.\n",
            "[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\n",
            "age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\n",
            "Recognition,pages770–778,2016.\n",
            "[12] SeppHochreiter,YoshuaBengio,PaoloFrasconi,andJürgenSchmidhuber. Gradientflowin\n",
            "recurrentnets: thedifficultyoflearninglong-termdependencies,2001.\n",
            "[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\n",
            "9(8):1735–1780,1997.\n",
            "[14] ZhongqiangHuangandMaryHarper. Self-trainingPCFGgrammarswithlatentannotations\n",
            "acrosslanguages. InProceedingsofthe2009ConferenceonEmpiricalMethodsinNatural\n",
            "LanguageProcessing,pages832–841.ACL,August2009.\n",
            "[15] RafalJozefowicz,OriolVinyals,MikeSchuster,NoamShazeer,andYonghuiWu. Exploring\n",
            "thelimitsoflanguagemodeling. arXivpreprintarXiv:1602.02410,2016.\n",
            "[16] ŁukaszKaiserandSamyBengio. Canactivememoryreplaceattention? InAdvancesinNeural\n",
            "InformationProcessingSystems,(NIPS),2016.\n",
            "[17] ŁukaszKaiserandIlyaSutskever. NeuralGPUslearnalgorithms. InInternationalConference\n",
            "onLearningRepresentations(ICLR),2016.\n",
            "[18] NalKalchbrenner,LasseEspeholt,KarenSimonyan,AaronvandenOord,AlexGraves,andKo-\n",
            "rayKavukcuoglu.Neuralmachinetranslationinlineartime.arXivpreprintarXiv:1610.10099v2,\n",
            "2017.\n",
            "[19] YoonKim,CarlDenton,LuongHoang,andAlexanderM.Rush. Structuredattentionnetworks.\n",
            "InInternationalConferenceonLearningRepresentations,2017.\n",
            "[20] DiederikKingmaandJimmyBa. Adam: Amethodforstochasticoptimization. InICLR,2015.\n",
            "[21] OleksiiKuchaievandBorisGinsburg. FactorizationtricksforLSTMnetworks. arXivpreprint\n",
            "arXiv:1703.10722,2017.\n",
            "[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\n",
            "Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\n",
            "arXiv:1703.03130,2017.\n",
            "[23] Minh-ThangLuong,QuocV.Le,IlyaSutskever,OriolVinyals,andLukaszKaiser. Multi-task\n",
            "sequencetosequencelearning. arXivpreprintarXiv:1511.06114,2015.\n",
            "[24] Minh-ThangLuong,HieuPham,andChristopherDManning. Effectiveapproachestoattention-\n",
            "basedneuralmachinetranslation. arXivpreprintarXiv:1508.04025,2015.\n",
            "11[25] MitchellPMarcus,MaryAnnMarcinkiewicz,andBeatriceSantorini.Buildingalargeannotated\n",
            "corpusofenglish: Thepenntreebank. Computationallinguistics,19(2):313–330,1993.\n",
            "[26] DavidMcClosky,EugeneCharniak,andMarkJohnson. Effectiveself-trainingforparsing. In\n",
            "ProceedingsoftheHumanLanguageTechnologyConferenceoftheNAACL,MainConference,\n",
            "pages152–159.ACL,June2006.\n",
            "[27] AnkurParikh,OscarTäckström,DipanjanDas,andJakobUszkoreit. Adecomposableattention\n",
            "model. InEmpiricalMethodsinNaturalLanguageProcessing,2016.\n",
            "[28] RomainPaulus,CaimingXiong,andRichardSocher. Adeepreinforcedmodelforabstractive\n",
            "summarization. arXivpreprintarXiv:1705.04304,2017.\n",
            "[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\n",
            "and interpretable tree annotation. In Proceedings of the 21st International Conference on\n",
            "ComputationalLinguisticsand44thAnnualMeetingoftheACL,pages433–440.ACL,July\n",
            "2006.\n",
            "[30] OfirPressandLiorWolf. Usingtheoutputembeddingtoimprovelanguagemodels. arXiv\n",
            "preprintarXiv:1608.05859,2016.\n",
            "[31] RicoSennrich,BarryHaddow,andAlexandraBirch. Neuralmachinetranslationofrarewords\n",
            "withsubwordunits. arXivpreprintarXiv:1508.07909,2015.\n",
            "[32] NoamShazeer,AzaliaMirhoseini,KrzysztofMaziarz,AndyDavis,QuocLe,GeoffreyHinton,\n",
            "andJeffDean. Outrageouslylargeneuralnetworks: Thesparsely-gatedmixture-of-experts\n",
            "layer. arXivpreprintarXiv:1701.06538,2017.\n",
            "[33] NitishSrivastava,GeoffreyEHinton,AlexKrizhevsky,IlyaSutskever,andRuslanSalakhutdi-\n",
            "nov. Dropout: asimplewaytopreventneuralnetworksfromoverfitting. JournalofMachine\n",
            "LearningResearch,15(1):1929–1958,2014.\n",
            "[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\n",
            "networks. InC.Cortes, N.D.Lawrence, D.D.Lee, M.Sugiyama, andR.Garnett, editors,\n",
            "AdvancesinNeuralInformationProcessingSystems28,pages2440–2448.CurranAssociates,\n",
            "Inc.,2015.\n",
            "[35] IlyaSutskever,OriolVinyals,andQuocVVLe. Sequencetosequencelearningwithneural\n",
            "networks. InAdvancesinNeuralInformationProcessingSystems,pages3104–3112,2014.\n",
            "[36] ChristianSzegedy,VincentVanhoucke,SergeyIoffe,JonathonShlens,andZbigniewWojna.\n",
            "Rethinkingtheinceptionarchitectureforcomputervision. CoRR,abs/1512.00567,2015.\n",
            "[37] Vinyals&Kaiser, Koo, Petrov, Sutskever, andHinton. Grammarasaforeignlanguage. In\n",
            "AdvancesinNeuralInformationProcessingSystems,2015.\n",
            "[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\n",
            "Macherey,MaximKrikun,YuanCao,QinGao,KlausMacherey,etal. Google’sneuralmachine\n",
            "translationsystem: Bridgingthegapbetweenhumanandmachinetranslation. arXivpreprint\n",
            "arXiv:1609.08144,2016.\n",
            "[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\n",
            "fast-forwardconnectionsforneuralmachinetranslation. CoRR,abs/1606.04199,2016.\n",
            "[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\n",
            "shift-reduceconstituentparsing. InProceedingsofthe51stAnnualMeetingoftheACL(Volume\n",
            "1: LongPapers),pages434–443.ACL,August2013.\n",
            "12Input-Input Layer5\n",
            "AttentionVisualizations\n",
            "stnemnrevog\n",
            "noitartsiger\n",
            "naciremA\n",
            "siht tirips taht ytirojam evah dessap wen swal ecnis 9002 gnikam eht gnitov ssecorp erom tluciffid >SOE< >dap< >dap< >dap< >dap< >dap< >dap<\n",
            "tI si ni a fo ro .\n",
            "tI si ni siht tirips taht a ytirojam fo naciremA stnemnrevog evah dessap wen swal ecnis 9002 gnikam eht noitartsiger ro gnitov ssecorp erom tluciffid . >SOE< >dap< >dap< >dap< >dap< >dap< >dap<\n",
            "Figure 3: An example of the attention mechanism following long-distance dependencies in the\n",
            "encoderself-attentioninlayer5of6. Manyoftheattentionheadsattendtoadistantdependencyof\n",
            "theverb‘making’,completingthephrase‘making...moredifficult’. Attentionshereshownonlyfor\n",
            "theword‘making’. Differentcolorsrepresentdifferentheads. Bestviewedincolor.\n",
            "13Input-Input Layer5\n",
            "noitacilppa\n",
            "tcefrep dluohs gnissim noinipo >SOE< >dap<\n",
            "reven\n",
            "ehT waL lliw eb tub sti eb tsuj siht si tahw ew era ni ym\n",
            ", - , .\n",
            "ehT waL lliw reven eb tcefrep , tub sti noitacilppa dluohs eb tsuj - siht si tahw ew era gnissim , ni ym noinipo . >SOE< >dap<\n",
            "Input-Input Layer5\n",
            "noitacilppa\n",
            "tcefrep dluohs gnissim noinipo >SOE< >dap<\n",
            "reven\n",
            "ehT waL lliw eb tub sti eb tsuj siht si tahw ew era ni ym\n",
            ", - , .\n",
            "ehT waL lliw reven eb tcefrep , tub sti noitacilppa dluohs eb tsuj - siht si tahw ew era gnissim , ni ym noinipo . >SOE< >dap<\n",
            "Figure4: Twoattentionheads,alsoinlayer5of6,apparentlyinvolvedinanaphoraresolution. Top:\n",
            "Fullattentionsforhead5. Bottom: Isolatedattentionsfromjusttheword‘its’forattentionheads5\n",
            "and6. Notethattheattentionsareverysharpforthisword.\n",
            "14Input-Input Layer5\n",
            "noitacilppa\n",
            "tcefrep dluohs gnissim noinipo >SOE< >dap<\n",
            "reven\n",
            "ehT waL lliw eb tub sti eb tsuj siht si tahw ew era ni ym\n",
            ", - , .\n",
            "ehT waL lliw reven eb tcefrep , tub sti noitacilppa dluohs eb tsuj - siht si tahw ew era gnissim , ni ym noinipo . >SOE< >dap<\n",
            "Input-Input Layer5\n",
            "noitacilppa\n",
            "tcefrep dluohs gnissim noinipo >SOE< >dap<\n",
            "reven\n",
            "ehT waL lliw eb tub sti eb tsuj siht si tahw ew era ni ym\n",
            ", - , .\n",
            "ehT waL lliw reven eb tcefrep , tub sti noitacilppa dluohs eb tsuj - siht si tahw ew era gnissim , ni ym noinipo . >SOE< >dap<\n",
            "Figure5: Manyoftheattentionheadsexhibitbehaviourthatseemsrelatedtothestructureofthe\n",
            "sentence. Wegivetwosuchexamplesabove,fromtwodifferentheadsfromtheencoderself-attention\n",
            "atlayer5of6. Theheadsclearlylearnedtoperformdifferenttasks.\n",
            "15\n"
          ]
        }
      ],
      "source": [
        "import pdfplumber\n",
        "\n",
        "# Specify the path to your PDF file\n",
        "pdf_file_path = \"/content/Ghost in the Wires_ My Adventures as the World's Most Wanted Hacker.pdf\"\n",
        "\n",
        "# Open the PDF file using pdfplumber\n",
        "with pdfplumber.open(pdf_file_path) as pdf:\n",
        "    # Extract text from each page\n",
        "    pdf_text = \"\"\n",
        "    for page in pdf.pages:\n",
        "        pdf_text += page.extract_text()\n",
        "\n",
        "# You can then print or process the extracted text as needed\n",
        "print(pdf_text)\n",
        "\n",
        "#!pip install unstructured[pdf]\n",
        "#from unstructured import extract_text_from_pdf\n",
        "\n",
        "##research_paper_content = extract_text_from_pdf(research_paper_path)\n",
        "\n",
        "# You can then print or process the extracted text as needed\n",
        "#print(pdf_text)\n",
        "\n",
        "\n",
        "#article_information_content = extract_text_from_image(article_information_path)\n",
        "\n",
        "\n",
        "#research_paper_content = extract_file_content(research_paper_path)\n",
        "#article_information_content = extract_file_content(article_information_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "9NOX4YwgbRJJ",
        "outputId": "7aab5cd5-4c14-4863-d1e5-4468ea3d2c10"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-868c2bb11f77>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Extract text from the research paper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresearch_paper_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_text_from_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Extract text from the article information document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#article_information_content = extract_text_from_pdf(article_information_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-0f43033ab537>\u001b[0m in \u001b[0;36mextract_text_from_pdf\u001b[0;34m(pdf_file)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnstructuredFileLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mpdf_pages_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/document_loaders/unstructured.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;34m\"\"\"Load file.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0melements\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_process_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"elements\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/document_loaders/unstructured.py\u001b[0m in \u001b[0;36m_get_elements\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0munstructured\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munstructured_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/unstructured/partition/auto.py\u001b[0m in \u001b[0;36mpartition\u001b[0;34m(filename, content_type, file, file_filename, url, include_page_breaks, strategy, encoding, paragraph_grouper, headers, skip_infer_table_types, ssl_verify, ocr_languages, pdf_infer_table_structure, xml_keep_tags, data_source_metadata, metadata_filename, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m         )\n\u001b[1;32m    291\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mfiletype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFileType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPDF\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0m_partition_pdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_partition_with_extras\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pdf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         elements = _partition_pdf(\n\u001b[1;32m    294\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/unstructured/partition/auto.py\u001b[0m in \u001b[0;36m_get_partition_with_extras\u001b[0;34m(doc_type, partition_with_extras_map)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0m_partition_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartition_with_extras_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_partition_func\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         raise ImportError(\n\u001b[0m\u001b[1;32m    111\u001b[0m             \u001b[0;34mf\"partition_{doc_type} is not available. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;34mf\"Install the {doc_type} dependencies with \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: partition_pdf is not available. Install the pdf dependencies with pip install \"unstructured[pdf]\"",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Extract text from the research paper\n",
        "research_paper_content = extract_text_from_pdf(pdf_file_path)\n",
        "\n",
        "# Extract text from the article information document\n",
        "#article_information_content = extract_text_from_pdf(article_information_path)\n",
        "\n",
        "# Define the number of characters to display\n",
        "nb_characters = 400\n",
        "\n",
        "# Print the first 400 characters of both documents\n",
        "print(f\"First {nb_characters} Characters of the Paper: \\n{research_paper_content[:nb_characters]}...\")\n",
        "print(\"---\" * 5)\n",
        "#print(f\"First {nb_characters} Characters of Article Information Document:\\n{article_information_content[:nb_characters]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pe9SbHA0bRJK"
      },
      "source": [
        "## Chat Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NS3f3DHbRJK"
      },
      "source": [
        "### Create Chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "Do5CqRBPbRJK"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator = \"\\n\\n\",\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap  = 200,\n",
        "    length_function = len,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeieycWHbRJK",
        "outputId": "d89ae168-38d9-4d2a-ee75-e8b15c2bb14b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Chunks in Research Paper: 1\n"
          ]
        }
      ],
      "source": [
        "research_paper_chunks = text_splitter.split_text(pdf_file_path)\n",
        "#article_information_chunks = text_splitter.split_text(article_information_content)\n",
        "\n",
        "print(f\"# Chunks in Research Paper: {len(research_paper_chunks)}\")\n",
        "#print(f\"# Chunks in Article Document: {len(article_information_chunks)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHADUIJ_bRJL"
      },
      "source": [
        "### Create Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "2lg6rMf7bRJL"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "\n",
        "embeddings = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrpCN-e2bRJL"
      },
      "source": [
        "### Create Vector Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "7dB5DiHObRJL"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "def get_doc_search(text_splitter):\n",
        "\n",
        "    return FAISS.from_texts(text_splitter, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "rjlgLedsbRJL",
        "outputId": "fe96a085-9e4f-4b9d-aaf2-d0ad1b47d6a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 8.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 10.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-c455a8b10f02>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdoc_search_paper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_doc_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresearch_paper_chunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_search_paper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-70ce1d377626>\u001b[0m in \u001b[0;36mget_doc_search\u001b[0;34m(text_splitter)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_doc_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_splitter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFAISS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_splitter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/vectorstores/faiss.py\u001b[0m in \u001b[0;36mfrom_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m                 \u001b[0mfaiss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFAISS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \"\"\"\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         return cls.__from(\n\u001b[1;32m    604\u001b[0m             \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/embeddings/openai.py\u001b[0m in \u001b[0;36membed_documents\u001b[0;34m(self, texts, chunk_size)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;31m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;31m#       than the maximum context and use length-safe embedding function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_len_safe_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeployment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m     async def aembed_documents(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/embeddings/openai.py\u001b[0m in \u001b[0;36m_get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m             response = embed_with_retry(\n\u001b[0m\u001b[1;32m    368\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_chunk_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/embeddings/openai.py\u001b[0m in \u001b[0;36membed_with_retry\u001b[0;34m(embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_check_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_empty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_empty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_embed_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mwrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mretry_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mWrappedFn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mretry_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetryCallState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0mretry_exc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry_error_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                     \u001b[0mretry_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/embeddings/openai.py\u001b[0m in \u001b[0;36m_embed_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mretry_decorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_embed_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_check_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_empty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_empty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/embedding.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0;31m# If a user specifies base64, we'll just return the encoded string.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    766\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             )\n",
            "\u001b[0;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details."
          ]
        }
      ],
      "source": [
        "doc_search_paper = get_doc_search(research_paper_chunks)\n",
        "print(doc_search_paper)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1g2l8UzbRJM"
      },
      "source": [
        "### Start chatting with your document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "dNA4TsHpu6OM"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "chain = load_qa_chain(OpenAI(), chain_type = \"map_rerank\",\n",
        "                      return_intermediate_steps=True)\n",
        "\n",
        "def chat_with_file(file_path, query):\n",
        "\n",
        "    file_content = extract_file_content(file_path)\n",
        "    file_splitter = text_splitter.split_text(file_content)\n",
        "\n",
        "    document_search = get_doc_search(file_splitter)\n",
        "    documents = document_search.similarity_search(query)\n",
        "\n",
        "    results = chain({\n",
        "                        \"input_documents\":documents,\n",
        "                        \"question\": query\n",
        "                    },\n",
        "                    return_only_outputs=True)\n",
        "    results = results['intermediate_steps'][0]\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7r2m0NVbRJN"
      },
      "source": [
        "##### Chat with the PDF file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "X5VtFol6bRJN",
        "outputId": "c194be68-a55a-4c31-acf6-37e0479e23a5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-9cdf805262e8>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Why is the self-attention approach used in this document?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchat_with_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresearch_paper_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"answer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-55-9e1a03eda1c8>\u001b[0m in \u001b[0;36mchat_with_file\u001b[0;34m(file_path, query)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mchat_with_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mfile_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_file_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mfile_splitter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_splitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-0f43033ab537>\u001b[0m in \u001b[0;36mextract_file_content\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnstructuredImageLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mdocuments_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/document_loaders/unstructured.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;34m\"\"\"Load file.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0melements\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_process_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"elements\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/document_loaders/unstructured.py\u001b[0m in \u001b[0;36m_get_elements\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0munstructured\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munstructured_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/unstructured/partition/auto.py\u001b[0m in \u001b[0;36mpartition\u001b[0;34m(filename, content_type, file, file_filename, url, include_page_breaks, strategy, encoding, paragraph_grouper, headers, skip_infer_table_types, ssl_verify, ocr_languages, pdf_infer_table_structure, xml_keep_tags, data_source_metadata, metadata_filename, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m         )\n\u001b[1;32m    291\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mfiletype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFileType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPDF\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0m_partition_pdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_partition_with_extras\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pdf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         elements = _partition_pdf(\n\u001b[1;32m    294\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/unstructured/partition/auto.py\u001b[0m in \u001b[0;36m_get_partition_with_extras\u001b[0;34m(doc_type, partition_with_extras_map)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0m_partition_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartition_with_extras_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_partition_func\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         raise ImportError(\n\u001b[0m\u001b[1;32m    111\u001b[0m             \u001b[0;34mf\"partition_{doc_type} is not available. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;34mf\"Install the {doc_type} dependencies with \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: partition_pdf is not available. Install the pdf dependencies with pip install \"unstructured[pdf]\"",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "query = \"Why is the self-attention approach used in this document?\"\n",
        "\n",
        "results = chat_with_file(research_paper_path, query)\n",
        "\n",
        "answer = results[\"answer\"]\n",
        "confidence_score = results[\"score\"]\n",
        "\n",
        "print(f\"Answer: {answer}\\n\\nConfidence Score: {confidence_score}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "pandas_benchmark",
      "language": "python",
      "name": "pandas_benchmark"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}